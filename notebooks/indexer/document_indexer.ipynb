{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Content Processor for Azure AI Search\n",
    "\n",
    "This notebook provides tools for processing PDF documents and preparing them for Azure AI Search. It includes functionality for:\n",
    "- PDF text extraction using PyMuPDF\n",
    "- Content chunking and processing\n",
    "- Embedding generation using Azure OpenAI\n",
    "- Azure AI Search index creation and population\n",
    "\n",
    "## Setup and Dependencies\n",
    "First, let's import all required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import dotenv\n",
    "import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Azure and OpenAI related imports\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ScoringProfile,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    TextWeights,\n",
    ")\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Set up the paths and configuration parameters for document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory configuration\n",
    "BASE_DIR = \"../doc/Temario\"\n",
    "PDF_DIR = os.path.join(BASE_DIR, \"temas_por_secciones\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"../data/output\")\n",
    "METADATA_FILE = os.path.join(PDF_DIR, \"document_label.yaml\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load metadata from YAML file\n",
    "with open(METADATA_FILE, 'r') as file:\n",
    "    metadata = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing Functions\n",
    "Core functions for extracting and processing content from PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(pdf_dir: str, metadata: dict) -> list:\n",
    "    \"\"\"\n",
    "    Extract text content from PDF files in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir (str): Directory containing PDF files\n",
    "        metadata (dict): Dictionary containing metadata for each PDF\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing extracted text and metadata for each page\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if not filename.endswith(\".pdf\"):\n",
    "            continue\n",
    "            \n",
    "        path = os.path.join(pdf_dir, filename)\n",
    "        doc = fitz.open(path)\n",
    "        \n",
    "        # Get metadata for the current file\n",
    "        metadata_key = filename.split('.')[0]\n",
    "        metadata_info = metadata.get(metadata_key, {})\n",
    "        skills = metadata_info.get('skills', [])\n",
    "        if not isinstance(skills, list):\n",
    "            skills = [skills]\n",
    "            \n",
    "        subject = metadata_info.get('subject', 'Unknown Subject')\n",
    "        difficulty = metadata_info.get('difficulty', 'Unknown Difficulty')\n",
    "        description = metadata_info.get('description', 'No Description')\n",
    "\n",
    "        # Process each page\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Extract chapter information from first page\n",
    "            chapter_number = None\n",
    "            chapter_title = None\n",
    "            if page_num == 0:\n",
    "                match_page = re.search(r\"\\b(\\d{3})\\b\", text)\n",
    "                match_title = re.search(r\"C\\nH\\nA\\nP\\nT\\nE\\nR\\n(.*?)\\nCONTENTS\", text, re.DOTALL)\n",
    "                \n",
    "                chapter_number = int(match_page.group(1)) if match_page else None\n",
    "                chapter_title = match_title.group(1).strip() if match_title else None\n",
    "\n",
    "            # Create document entry\n",
    "            documents.append({\n",
    "                'filename': filename,\n",
    "                'page_number': chapter_number + page_num if chapter_number else page_num + 1,\n",
    "                'text': text,\n",
    "                'skills': skills,\n",
    "                'subject': subject,\n",
    "                'difficulty': difficulty,\n",
    "                'description': description,\n",
    "                'chapter_title': chapter_title\n",
    "            })\n",
    "            \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing and Chunking\n",
    "Process the extracted documents and split them into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(pages: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Convert raw pages into LangChain documents and split them into chunks.\n",
    "    \n",
    "    Args:\n",
    "        pages (list): List of page dictionaries from PDF processing\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (LangChain documents, chunks)\n",
    "    \"\"\"\n",
    "    # Convert pages to LangChain documents\n",
    "    docs = []\n",
    "    for page in pages:\n",
    "        docs.append(Document(\n",
    "            page_content=page[\"text\"], \n",
    "            metadata={\n",
    "                \"page_number\": str(page[\"page_number\"]), \n",
    "                \"filename\": str(page[\"filename\"]),\n",
    "                \"skills\": page[\"skills\"],\n",
    "                \"subject\": page[\"subject\"],\n",
    "                \"difficulty\": page[\"difficulty\"],\n",
    "                \"description\": page[\"description\"],\n",
    "                \"chapter_title\": page[\"chapter_title\"]\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1300,\n",
    "        chunk_overlap=100,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    return docs, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Search Setup\n",
    "Configure and create the Azure AI Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index_schema(embedding_dimension: int) -> list:\n",
    "    \"\"\"\n",
    "    Create the schema for Azure AI Search index.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dimension (int): Dimension of the embedding vectors\n",
    "    \n",
    "    Returns:\n",
    "        list: List of field definitions for the search index\n",
    "    \"\"\"\n",
    "    return [\n",
    "        SimpleField(\n",
    "            name=\"id\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            key=True,\n",
    "            filterable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"content\", \n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True\n",
    "        ),\n",
    "        SearchableField(name=\"metadata\", \n",
    "                        type=SearchFieldDataType.String, \n",
    "                        searchable=True),\n",
    "        SearchField(\n",
    "            name=\"content_vector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=embedding_dimension,\n",
    "            vector_search_profile_name=\"myHnswProfile\"\n",
    "        ),\n",
    "        SimpleField(\n",
    "            name=\"page_number\",\n",
    "            type=SearchFieldDataType.Int32,\n",
    "            filterable=True,\n",
    "            facetable=True,\n",
    "            sortable=True\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"skills\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "            collection=True\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"subject\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"difficulty\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"description\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"filename\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True\n",
    "        ),\n",
    "        SimpleField(\n",
    "            name=\"start_index\",\n",
    "            type=SearchFieldDataType.Int32,\n",
    "            searchable=True\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline\n",
    "Execute the complete document processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDFs...\n",
      "Processing documents and creating chunks...\n",
      "Creating and populating Azure AI Search index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [01:08<01:08, 68.17s/it]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main processing pipeline for PDF documents.\"\"\"\n",
    "    # Initialize Azure OpenAI embeddings\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "        openai_api_version=\"2024-02-01\"\n",
    "    )\n",
    "    \n",
    "    # Extract text from PDFs\n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    pages = extract_text_from_pdfs(PDF_DIR, metadata)\n",
    "    \n",
    "    # Process documents and create chunks\n",
    "    print(\"Processing documents and creating chunks...\")\n",
    "    docs, chunks = process_documents(pages)\n",
    "    \n",
    "    # Generate embeddings for each page\n",
    "    #print(\"Generating embeddings...\")\n",
    "    #for page in tqdm.tqdm(pages):\n",
    "    #    page[\"embedding\"] = embeddings.embed_query(page[\"text\"])\n",
    "    \n",
    "    \n",
    "    # Create and populate Azure AI Search index\n",
    "    print(\"Creating and populating Azure AI Search index...\")\n",
    "    index_name = \"temario-index-v1\"\n",
    "    vector_store = AzureSearch(\n",
    "        embedding_function=embeddings.embed_query,\n",
    "        azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "        azure_search_key=os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    "        index_name=index_name,\n",
    "        additional_search_client_options={\"retry_total\": 3},\n",
    "        fields=create_search_index_schema(len(embeddings.embed_query(\"test\")))\n",
    "    )\n",
    "    \n",
    "    # Add chunks to vector store in batches\n",
    "    batch_size = 500\n",
    "    for i in tqdm.tqdm(range(0, len(chunks), batch_size)):\n",
    "        content_batch = [chunk.page_content for chunk in chunks[i:i + batch_size]]\n",
    "        metadata_batch = [chunk.metadata for chunk in chunks[i:i + batch_size]]\n",
    "        vector_store.add_texts(texts=content_batch, metadatas=metadata_batch)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "# Execute the main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Search Index\n",
    "Test the created search index with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search(query: str, vector_store: AzureSearch):\n",
    "    \"\"\"\n",
    "    Test the search index with a sample query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        vector_store (AzureSearch): Initialized Azure Search instance\n",
    "    \"\"\"\n",
    "    results = vector_store.similarity_search(query)\n",
    "    \n",
    "    print(f\"Search results for query: '{query}'\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"Content: {result.page_content[:200]}...\")\n",
    "        print(f\"Page Number: {result.metadata['page_number']}\")\n",
    "        print(f\"Chapter Title: {result.metadata.get('chapter_title', 'N/A')}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Example search\n",
    "test_search(\"What is the main topic of the document?\", vector_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutoria",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
